{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead634e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vladagafonov/work/audio\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3c935e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(   tensor([[-0.0065, -0.0055, -0.0062,  ...,  0.0033,  0.0005, -0.0095]]),\n",
      "    16000,\n",
      "    'CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED '\n",
      "    'JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED '\n",
      "    'WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK',\n",
      "    103,\n",
      "    1240,\n",
      "    0)\n",
      "torch.Size([1, 225360])\n",
      "(   tensor([[-0.0059, -0.0045, -0.0067,  ...,  0.0007,  0.0034,  0.0047]]),\n",
      "    16000,\n",
      "    'THAT HAD ITS SOURCE AWAY BACK IN THE WOODS OF THE OLD CUTHBERT PLACE IT '\n",
      "    'WAS REPUTED TO BE AN INTRICATE HEADLONG BROOK IN ITS EARLIER COURSE '\n",
      "    'THROUGH THOSE WOODS WITH DARK SECRETS OF POOL AND CASCADE BUT BY THE TIME '\n",
      "    \"IT REACHED LYNDE'S HOLLOW IT WAS A QUIET WELL CONDUCTED LITTLE STREAM\",\n",
      "    103,\n",
      "    1240,\n",
      "    1)\n",
      "torch.Size([1, 255120])\n"
     ]
    }
   ],
   "source": [
    "# import torchaudio\n",
    "\n",
    "# waveform, sample_rate = torchaudio.load('audio/foo.wav')  # load tensor from file\n",
    "# torchaudio.save('foo_save.wav', waveform, sample_rate)  # save tensor to file\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import torch\n",
    "import pickle\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torchaudio.datasets.librispeech import LIBRISPEECH\n",
    "\n",
    "from examples.self_supervised_learning.data_modules._utils import BucketizeBatchSampler, CollateFnWav2Vec2, DistributedBatchSampler, HuBERTDataSet\n",
    "from torchaudio.prototype.models._conformer_wav2vec2 import * \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "librispeech_cls = LIBRISPEECH\n",
    "dataset = librispeech_cls(\"..\", url=\"train-clean-100\", download=False)\n",
    "pp.pprint(dataset[0])\n",
    "pp.pprint(dataset[0][0].shape)\n",
    "\n",
    "pp.pprint(dataset[1])\n",
    "pp.pprint(dataset[1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77afaf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 205, 64])\n",
      "tensor([[[4.9948e-05, 5.1273e-05, 5.1331e-05,  ..., 1.2706e-05,\n",
      "          1.5056e-05, 1.7696e-05],\n",
      "         [2.3116e-04, 3.1802e-04, 5.3764e-04,  ..., 7.8896e-06,\n",
      "          1.1558e-05, 1.5129e-05],\n",
      "         [1.7193e-04, 1.7902e-04, 1.8610e-04,  ..., 1.1580e-05,\n",
      "          1.4466e-05, 1.1392e-05],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[1.6334e-03, 1.9052e-03, 2.5277e-03,  ..., 6.6480e-04,\n",
      "          9.5149e-04, 1.2171e-04],\n",
      "         [1.0273e-02, 1.1684e-02, 1.4787e-02,  ..., 5.8813e-04,\n",
      "          7.4887e-04, 1.8714e-04],\n",
      "         [1.5773e-04, 1.2574e-03, 4.2343e-03,  ..., 3.4065e-04,\n",
      "          6.9049e-04, 2.9644e-04],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[4.9698e-01, 3.7330e-01, 1.9749e-03,  ..., 1.9058e-05,\n",
      "          4.2935e-05, 3.0420e-05],\n",
      "         [8.2210e-01, 6.2357e-01, 2.5794e-02,  ..., 1.8725e-05,\n",
      "          3.1274e-05, 2.9540e-05],\n",
      "         [6.0832e-01, 4.5877e-01, 9.2681e-03,  ..., 4.7592e-04,\n",
      "          5.3292e-04, 5.1389e-04],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.6420e-03, 3.9239e-03, 4.4314e-03,  ..., 2.6423e-05,\n",
      "          3.9564e-05, 3.4931e-05],\n",
      "         [2.1164e-04, 2.8904e-04, 4.8435e-04,  ..., 4.9994e-05,\n",
      "          4.8576e-05, 5.0215e-05],\n",
      "         [1.6033e-04, 2.1102e-04, 3.3739e-04,  ..., 3.0462e-05,\n",
      "          4.7682e-05, 7.2920e-05],\n",
      "         ...,\n",
      "         [5.6799e-04, 1.1631e-03, 2.7398e-03,  ..., 3.9602e-05,\n",
      "          6.0808e-05, 7.8115e-05],\n",
      "         [1.0891e-03, 1.0257e-03, 7.7594e-04,  ..., 4.2011e-05,\n",
      "          3.9585e-05, 3.4660e-05],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[3.2672e-03, 2.4661e-03, 5.7219e-05,  ..., 1.0185e-05,\n",
      "          4.0977e-06, 1.4718e-06],\n",
      "         [3.8664e-04, 3.7017e-04, 2.9797e-04,  ..., 2.3739e-06,\n",
      "          2.6662e-06, 1.7123e-06],\n",
      "         [3.6329e-04, 3.2032e-04, 1.7778e-04,  ..., 4.3239e-06,\n",
      "          3.2622e-06, 2.5641e-06],\n",
      "         ...,\n",
      "         [4.6945e-03, 3.8929e-03, 1.3817e-03,  ..., 5.5010e-04,\n",
      "          3.4424e-04, 3.1093e-04],\n",
      "         [2.3916e-02, 1.8149e-02, 7.8060e-04,  ..., 5.5475e-04,\n",
      "          3.2399e-04, 2.9170e-04],\n",
      "         [2.3893e-02, 1.7929e-02, 2.7935e-05,  ..., 5.8345e-04,\n",
      "          1.6503e-04, 2.6268e-04]],\n",
      "\n",
      "        [[1.1869e-02, 3.0245e-02, 7.9335e-02,  ..., 2.1387e-05,\n",
      "          8.3256e-06, 1.2390e-05],\n",
      "         [4.2026e-03, 5.6971e-03, 9.4599e-03,  ..., 2.5378e-05,\n",
      "          2.4460e-05, 2.3124e-05],\n",
      "         [2.8578e-02, 5.1756e-02, 1.1271e-01,  ..., 2.2039e-05,\n",
      "          3.6261e-05, 2.5799e-05],\n",
      "         ...,\n",
      "         [2.4750e-03, 3.0292e-03, 4.3596e-03,  ..., 2.2812e-05,\n",
      "          2.4247e-05, 1.9432e-05],\n",
      "         [2.7016e-03, 3.6168e-03, 5.9117e-03,  ..., 2.0574e-05,\n",
      "          4.9661e-05, 3.0462e-05],\n",
      "         [6.4988e-03, 5.3505e-03, 1.7692e-03,  ..., 3.5438e-05,\n",
      "          7.5381e-06, 3.3123e-05]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# len_list = [d[0].size(1) for d in dataset]\n",
    "# with open('len_list.obj', 'wb') as fp:\n",
    "#     pickle.dump(len_list, fp)\n",
    "    \n",
    "len_list = {}\n",
    "with open('../len_list.obj', 'rb') as fp:\n",
    "    len_list = pickle.load(fp)\n",
    "\n",
    "sampler = BucketizeBatchSampler(\n",
    "    len_list,\n",
    "    num_buckets=10000,\n",
    "    max_token_count=30 * 16000,\n",
    "    min_len=32000,\n",
    "    max_len=250000,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_sampler=sampler,\n",
    "            collate_fn=CollateFnWav2Vec2(),\n",
    "        )\n",
    "\n",
    "# for k in dataloader:\n",
    "#     print(k)\n",
    "#     break\n",
    "\n",
    "for X, y in dataloader:\n",
    "    print(X[0].shape)\n",
    "    print(X[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "204b599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conformer_wav2vec2_pretrain_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91e6fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 51, 256])\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataloader:\n",
    "    y_t = model(*X)\n",
    "    print(y_t[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd543474",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.wav2vec2.named_buffers():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d9282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 205, 64])\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataloader:\n",
    "    print(X[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba094f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = torch.Size([14, 205, 64])\n",
      "x = torch.Size([14, 51, 256])\n",
      "lengths = torch.Size([14]) tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51],\n",
      "       dtype=torch.int32)\n",
      "mask_indices = torch.Size([14, 51])\n",
      "targets = torch.Size([14, 51, 256])\n",
      "negatives = torch.Size([100, 14, 51, 256])\n",
      "neg_idxs = torch.Size([14, 5100]) tensor([[  2,  33,  17,  ...,  22,  16,  43],\n",
      "        [ 92,  75,  83,  ...,  93,  86,  96],\n",
      "        [140, 109, 132,  ..., 128, 113, 125],\n",
      "        ...,\n",
      "        [577, 563, 569,  ..., 585, 577, 562],\n",
      "        [660, 618, 638,  ..., 612, 628, 648],\n",
      "        [682, 683, 676,  ..., 690, 706, 682]])\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataloader:\n",
    "    #print(X[0].shape)\n",
    "    x, lengths, mask_indices, targets, negatives, neg_idxs = model(*X)\n",
    "    print('X =', X[0].shape)\n",
    "    print('x =', x.shape)\n",
    "    print('lengths =', lengths.shape, lengths)\n",
    "    print('mask_indices =', mask_indices.shape)\n",
    "    print('targets =', targets.shape)\n",
    "    print('negatives =', negatives.shape)\n",
    "    print('neg_idxs =', neg_idxs.shape, neg_idxs)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8230d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p8/7jpdjc4n1jl78ftzx25pyy440000gn/T/ipykernel_70653/3076397914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;31m#print(X[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0my_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_contrastive_loss(self, x, mask_indices, targets, neg_is_pos, reduce):\n",
    "    x = (\n",
    "        x[mask_indices]\n",
    "        .view(x.size(0), -1, x.size(-1))\n",
    "        .unsqueeze(0)\n",
    "        .expand(targets.shape)\n",
    "    )\n",
    "    logits = torch.cosine_similarity(x.float(), targets.float(), dim=-1).float()\n",
    "    logits /= self.logit_temp\n",
    "    if neg_is_pos.any():\n",
    "        logits[1:][neg_is_pos] = float(\"-inf\")\n",
    "    target = logits.new_zeros(logits.size(1) * logits.size(2), dtype=torch.long)\n",
    "    logits = logits.transpose(0, 2)\n",
    "    logits = logits.reshape(-1, logits.size(-1))\n",
    "    loss = F.cross_entropy(\n",
    "        logits,\n",
    "        target,\n",
    "        reduction=\"sum\" if reduce else \"none\",\n",
    "    )\n",
    "    sample_size = target.numel()\n",
    "    return loss, sample_size, logits\n",
    "\n",
    "def wav2vec_loss(net_output, reduce=True):\n",
    "    \n",
    "    x, lengths, mask_indices, y, negatives, neg_idxs = net_output\n",
    "    print('x =', x.shape)\n",
    "    print('lengths =', lengths.shape, lengths)\n",
    "    print('mask_indices =', mask_indices.shape)\n",
    "    print('y =', y.shape)\n",
    "    print('negatives =', negatives.shape)\n",
    "    print('neg_idxs =', neg_idxs.shape, neg_idxs)\n",
    "\n",
    "    # 1. get x (note that, right_context is removed in net_output if there is right_context in src_tokens)\n",
    "    #x, lengths, state, extra_output = net_output\n",
    "\n",
    "    # 2. get negative samples\n",
    "    #negatives = extra_output[f\"extra_output_{self.masking_layer_key}\"]\n",
    "\n",
    "    # 3. get y and mask_indices\n",
    "    #y = self.get_registered_buffer(model, \"y_before_sampling\")\n",
    "    #mask_indices = self.get_registered_buffer(model, \"mask_indices\")\n",
    "    assert y is not None\n",
    "    assert mask_indices is not None\n",
    "    if model.training:\n",
    "        assert y.requires_grad\n",
    "   # assert mask_indices.sum() == y.shape[0] * y.shape[1]\n",
    "\n",
    "    # 4. compute targets\n",
    "    neg_is_pos = (y == negatives).all(-1)\n",
    "    y = y.unsqueeze(0)\n",
    "    targets = torch.cat([y, negatives], dim=0)\n",
    "\n",
    "    # 5. compute losses\n",
    "    loss, sample_size, logits = compute_contrastive_loss(\n",
    "        x, mask_indices, targets, neg_is_pos, reduce\n",
    "    )\n",
    "    loss = loss.float()\n",
    "\n",
    "    return loss, sample_size\n",
    "\n",
    "for X, y in dataloader:\n",
    "    #print(X[0].shape)\n",
    "    y_h = model(*X)\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    l = wav2vec_loss(y_h)\n",
    "    print(l[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d62dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc55ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = torch.Size([14, 51, 256])\n",
      "lengths = torch.Size([14]) tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51],\n",
      "       dtype=torch.int32)\n",
      "mask_indices = torch.Size([14, 51])\n",
      "targets = torch.Size([14, 51, 256])\n",
      "negatives = torch.Size([100, 14, 51, 256])\n",
      "neg_idxs = torch.Size([14, 5100]) tensor([[  6,  44,  33,  ...,  22,  31,  32],\n",
      "        [ 74,  91,  59,  ...,  67,  70,  74],\n",
      "        [141, 150, 129,  ..., 114, 146, 143],\n",
      "        ...,\n",
      "        [564, 568, 607,  ..., 579, 595, 597],\n",
      "        [660, 650, 651,  ..., 643, 643, 661],\n",
      "        [690, 679, 694,  ..., 707, 695, 699]])\n",
      "torch.Size([14, 51, 256])\n",
      "torch.Size([14, 12, 256])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 564 is out of bounds for dimension 0 with size 13",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p8/7jpdjc4n1jl78ftzx25pyy440000gn/T/ipykernel_48832/2895396777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0my_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m#  return x, lengths, mask_idxs, targets, negs, neg_idxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav2vec_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/p8/7jpdjc4n1jl78ftzx25pyy440000gn/T/ipykernel_48832/2895396777.py\u001b[0m in \u001b[0;36mwav2vec_loss\u001b[0;34m(net_output, reduce)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# 5. compute losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     loss, sample_size, _ = compute_contrastive_loss(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     )\n",
      "\u001b[0;32m/var/folders/p8/7jpdjc4n1jl78ftzx25pyy440000gn/T/ipykernel_48832/2895396777.py\u001b[0m in \u001b[0;36mcompute_contrastive_loss\u001b[0;34m(x, mask_indices, targets, neg_is_pos, reduce, logit_temp)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlogit_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mneg_is_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneg_is_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 564 is out of bounds for dimension 0 with size 13"
     ]
    }
   ],
   "source": [
    "def compute_contrastive_loss(x, mask_indices, targets, neg_is_pos, reduce, logit_temp=0.1):\n",
    "    print(x.shape)\n",
    "    print(x[mask_indices].view(x.size(0), -1, x.size(-1)).shape)\n",
    "    x = (\n",
    "        x[mask_indices]\n",
    "        .view(x.size(0), -1, x.size(-1))\n",
    "    )\n",
    "    targets = (\n",
    "        targets[mask_indices]\n",
    "        .view(targets.size(0), -1, targets.size(-1))\n",
    "    )\n",
    "    logits = torch.cosine_similarity(x.float(), targets.float(), dim=-1).float()\n",
    "    logits /= logit_temp\n",
    "    if neg_is_pos.any():\n",
    "        logits[1:][neg_is_pos] = float(\"-inf\")\n",
    "    target = logits.new_zeros(logits.size(1) * logits.size(2), dtype=torch.long)\n",
    "    logits = logits.transpose(0, 2)\n",
    "    logits = logits.reshape(-1, logits.size(-1))\n",
    "    loss = F.cross_entropy(\n",
    "        logits,\n",
    "        target,\n",
    "        reduction=\"sum\" if reduce else \"none\",\n",
    "    )\n",
    "    sample_size = target.numel()\n",
    "    return loss, sample_size, logits\n",
    "    \n",
    "def wav2vec_loss(net_output, reduce=True):\n",
    "\n",
    "    #x, lengths, state, extra_output = net_output\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    x, lengths, mask_indices, targets, negatives, neg_idxs = net_output\n",
    "    print('x =', x.shape)\n",
    "    print('lengths =', lengths.shape, lengths)\n",
    "    print('mask_indices =', mask_indices.shape)\n",
    "    print('targets =', targets.shape)\n",
    "    print('negatives =', negatives.shape)\n",
    "    print('neg_idxs =', neg_idxs.shape, neg_idxs)\n",
    "    \n",
    "    #assert y is not None\n",
    "    assert mask_indices is not None\n",
    "    #if model.training:\n",
    "    #    assert y.requires_grad\n",
    "    #assert mask_indices.sum() == y.shape[0] * y.shape[1]\n",
    "\n",
    "    # 4. compute targets\n",
    "    # neg_is_pos = (y == negatives).all(-1)\n",
    "    # y = y.unsqueeze(0)\n",
    "    # targets = torch.cat([y, negatives], dim=0)\n",
    "\n",
    "    # 5. compute losses\n",
    "    loss, sample_size, _ = compute_contrastive_loss(\n",
    "        x, mask_indices, targets, neg_idxs, reduce\n",
    "    )\n",
    "    loss = loss.float()\n",
    "\n",
    "    return loss, sample_size\n",
    "\n",
    "for X, y in dataloader:\n",
    "    #print(X[0].shape)\n",
    "    y_h = model(*X)\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    l = wav2vec_loss(y_h)\n",
    "    print(l[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323839d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrastive_loss(x, mask_indices, targets, neg_is_pos, reduce, logit_temp=0.1):\n",
    "    print(x.shape)\n",
    "    print(x[mask_indices].view(x.size(0), -1, x.size(-1)).shape)\n",
    "    x = (\n",
    "        x[mask_indices]\n",
    "        .view(x.size(0), -1, x.size(-1))\n",
    "        .unsqueeze(0)\n",
    "        .expand(targets.shape)\n",
    "    )\n",
    "    logits = torch.cosine_similarity(x.float(), targets.float(), dim=-1).float()\n",
    "    logits /= logit_temp\n",
    "    if neg_is_pos.any():\n",
    "        logits[1:][neg_is_pos] = float(\"-inf\")\n",
    "    target = logits.new_zeros(logits.size(1) * logits.size(2), dtype=torch.long)\n",
    "    logits = logits.transpose(0, 2)\n",
    "    logits = logits.reshape(-1, logits.size(-1))\n",
    "    loss = F.cross_entropy(\n",
    "        logits,\n",
    "        target,\n",
    "        reduction=\"sum\" if reduce else \"none\",\n",
    "    )\n",
    "    sample_size = target.numel()\n",
    "    return loss, sample_size, logits\n",
    "    \n",
    "def wav2vec_loss(net_output, reduce=True):\n",
    "\n",
    "    #x, lengths, state, extra_output = net_output\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    x, lengths, mask_indices, targets, negatives, neg_idxs = net_output\n",
    "    print('x =', x.shape)\n",
    "    print('lengths =', lengths.shape, lengths)\n",
    "    print('mask_indices =', mask_indices.shape)\n",
    "    print('targets =', targets.shape)\n",
    "    print('negatives =', negatives.shape)\n",
    "    print('neg_idxs =', neg_idxs.shape, neg_idxs)\n",
    "    \n",
    "    #assert y is not None\n",
    "    assert mask_indices is not None\n",
    "    #if model.training:\n",
    "    #    assert y.requires_grad\n",
    "    #assert mask_indices.sum() == y.shape[0] * y.shape[1]\n",
    "\n",
    "    # 4. compute targets\n",
    "    # neg_is_pos = (y == negatives).all(-1)\n",
    "    # y = y.unsqueeze(0)\n",
    "    # targets = torch.cat([y, negatives], dim=0)\n",
    "\n",
    "    # 5. compute losses\n",
    "    loss, sample_size, _ = compute_contrastive_loss(\n",
    "        x, mask_indices, targets, neg_idxs, reduce\n",
    "    )\n",
    "    loss = loss.float()\n",
    "\n",
    "    return loss, sample_size\n",
    "\n",
    "for X, y in dataloader:\n",
    "    #print(X[0].shape)\n",
    "    y_h = model(*X)\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    l = wav2vec_loss(y_h)\n",
    "    print(l[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323839d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrastive_loss(x, mask_indices, targets, neg_is_pos, reduce, logit_temp=0.1):\n",
    "    print(x.shape)\n",
    "    print(x[mask_indices].view(x.size(0), -1, x.size(-1)).shape)\n",
    "    x = (\n",
    "        x[mask_indices]\n",
    "        .view(x.size(0), -1, x.size(-1))\n",
    "        .unsqueeze(0)\n",
    "        .expand(targets.shape)\n",
    "    )\n",
    "    logits = torch.cosine_similarity(x.float(), targets.float(), dim=-1).float()\n",
    "    logits /= logit_temp\n",
    "    if neg_is_pos.any():\n",
    "        logits[1:][neg_is_pos] = float(\"-inf\")\n",
    "    target = logits.new_zeros(logits.size(1) * logits.size(2), dtype=torch.long)\n",
    "    logits = logits.transpose(0, 2)\n",
    "    logits = logits.reshape(-1, logits.size(-1))\n",
    "    loss = F.cross_entropy(\n",
    "        logits,\n",
    "        target,\n",
    "        reduction=\"sum\" if reduce else \"none\",\n",
    "    )\n",
    "    sample_size = target.numel()\n",
    "    return loss, sample_size, logits\n",
    "    \n",
    "def wav2vec_loss(net_output, reduce=True):\n",
    "\n",
    "    #x, lengths, state, extra_output = net_output\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    x, lengths, mask_indices, targets, negatives, neg_idxs = net_output\n",
    "    print('x =', x.shape)\n",
    "    print('lengths =', lengths.shape, lengths)\n",
    "    print('mask_indices =', mask_indices.shape)\n",
    "    print('targets =', targets.shape)\n",
    "    print('negatives =', negatives.shape)\n",
    "    print('neg_idxs =', neg_idxs.shape, neg_idxs)\n",
    "    \n",
    "    #assert y is not None\n",
    "    assert mask_indices is not None\n",
    "    #if model.training:\n",
    "    #    assert y.requires_grad\n",
    "    #assert mask_indices.sum() == y.shape[0] * y.shape[1]\n",
    "\n",
    "    # 4. compute targets\n",
    "    # neg_is_pos = (y == negatives).all(-1)\n",
    "    # y = y.unsqueeze(0)\n",
    "    # targets = torch.cat([y, negatives], dim=0)\n",
    "\n",
    "    # 5. compute losses\n",
    "    loss, sample_size, _ = compute_contrastive_loss(\n",
    "        x, mask_indices, targets, neg_idxs, reduce\n",
    "    )\n",
    "    loss = loss.float()\n",
    "\n",
    "    return loss, sample_size\n",
    "\n",
    "for X, y in dataloader:\n",
    "    #print(X[0].shape)\n",
    "    y_h = model(*X)\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    l = wav2vec_loss(y_h)\n",
    "    print(l[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffa418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2vec_loss(x, lengths, mask_idxs, targets, negs, neg_idxs, mask_prob=0.5):\n",
    "    # compute dot product between x and targets\n",
    "    B, T, C = x.shape\n",
    "    targets_proj = targets.unsqueeze(-1)\n",
    "    x_proj = x.view(B, T, 1, C)\n",
    "    dots = torch.matmul(x_proj, targets_proj).squeeze(-1)\n",
    "\n",
    "    # compute dot product between x and negatives\n",
    "    negs_proj = negs.view(B, -1, C).transpose(1, 2)\n",
    "    neg_dots = torch.matmul(x_proj, negs_proj)\n",
    "\n",
    "    # compute contrastive loss\n",
    "    logit_targets = dots / 0.07\n",
    "    logit_negs = neg_dots / 0.07\n",
    "    logits = torch.cat([logit_targets, logit_negs], dim=1)\n",
    "    labels = torch.zeros(B, T + negs.shape[1]).to(logits.device).long()\n",
    "    labels[:, :T] = 1\n",
    "    loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "\n",
    "    # mask loss for masked tokens\n",
    "    mask = torch.zeros_like(labels, dtype=torch.float32)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :l] = 1\n",
    "    mask[:, mask_idxs] = mask_prob\n",
    "    mask_loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "    return mask_loss\n",
    "\n",
    "for X, y in dataloader:\n",
    "    #print(X[0].shape)\n",
    "    y_h = model(*X)\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    l = wav2vec_loss(*y_h)\n",
    "    print(l[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffa418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2vec_loss(x, lengths, mask_idxs, targets, negs, neg_idxs, mask_prob=0.5):\n",
    "    # compute dot product between x and targets\n",
    "    B, T, C = x.shape\n",
    "    targets_proj = targets.unsqueeze(-1)\n",
    "    x_proj = x.view(B, T, 1, C)\n",
    "    dots = torch.matmul(x_proj, targets_proj).squeeze(-1)\n",
    "\n",
    "    # compute dot product between x and negatives\n",
    "    negs_proj = negs.view(B, -1, C).transpose(1, 2)\n",
    "    neg_dots = torch.matmul(x_proj, negs_proj)\n",
    "\n",
    "    # compute contrastive loss\n",
    "    logit_targets = dots / 0.07\n",
    "    logit_negs = neg_dots / 0.07\n",
    "    logits = torch.cat([logit_targets, logit_negs], dim=1)\n",
    "    labels = torch.zeros(B, T + negs.shape[1]).to(logits.device).long()\n",
    "    labels[:, :T] = 1\n",
    "    loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "\n",
    "    # mask loss for masked tokens\n",
    "    mask = torch.zeros_like(labels, dtype=torch.float32)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :l] = 1\n",
    "    mask[:, mask_idxs] = mask_prob\n",
    "    mask_loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "    return mask_loss\n",
    "\n",
    "for X, y in dataloader:\n",
    "    #print(X[0].shape)\n",
    "    y_h = model(*X)\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    l = wav2vec_loss(*y_h)\n",
    "    print(l[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffa418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2vec_loss(x, lengths, mask_idxs, targets, negs, neg_idxs, mask_prob=0.5):\n",
    "    # compute dot product between x and targets\n",
    "    B, T, C = x.shape\n",
    "    targets_proj = targets.unsqueeze(-1)\n",
    "    x_proj = x.view(B, T, 1, C)\n",
    "    dots = torch.matmul(x_proj, targets_proj).squeeze(-1)\n",
    "\n",
    "    # compute dot product between x and negatives\n",
    "    negs_proj = negs.view(B, -1, C).transpose(1, 2)\n",
    "    neg_dots = torch.matmul(x_proj, negs_proj)\n",
    "\n",
    "    # compute contrastive loss\n",
    "    logit_targets = dots / 0.07\n",
    "    logit_negs = neg_dots / 0.07\n",
    "    logits = torch.cat([logit_targets, logit_negs], dim=1)\n",
    "    labels = torch.zeros(B, T + negs.shape[1]).to(logits.device).long()\n",
    "    labels[:, :T] = 1\n",
    "    loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "\n",
    "    # mask loss for masked tokens\n",
    "    mask = torch.zeros_like(labels, dtype=torch.float32)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :l] = 1\n",
    "    mask[:, mask_idxs] = mask_prob\n",
    "    mask_loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "    return mask_loss\n",
    "\n",
    "for X, y in dataloader:\n",
    "    #print(X[0].shape)\n",
    "    y_h = model(*X)\n",
    "    #  return x, lengths, mask_idxs, targets, negs, neg_idxs\n",
    "    l = wav2vec_loss(*y_h)\n",
    "    print(l[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406b67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f84b3d992b7d009d4cc62272a60079a77e3afc79e4913fa3d8a99446f5153ed5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
